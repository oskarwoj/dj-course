# Engine configuration - which LLM to use
# Valid values: GEMINI, LLAMA_CPP
ENGINE=GEMINI

# Gemini API Configuration
GEMINI_API_KEY=your_gemini_api_key_here
MODEL_NAME=gemini-2.5-flash

# LLaMA Configuration (only needed if ENGINE=LLAMA_CPP)
LLAMA_MODEL_NAME=llama-3.1-8b-instruct
LLAMA_MODEL_PATH=/path/to/your/model.gguf
LLAMA_GPU_LAYERS=1
LLAMA_CONTEXT_SIZE=2048
